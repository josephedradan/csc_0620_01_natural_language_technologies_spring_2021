{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSC 0620-01 Natural Language Technologies Spring 2021\n",
    "\n",
    "Joseph Edradan <br>\n",
    "03/16/2021 <br>\n",
    "Source: https://www.kaggle.com/jayantawasthi/nlp-corona-tweet-with-random-forest-and-naivebayes <br>\n",
    "\n",
    "#### Submission for hands-on workshop on Mar 11th\n",
    "\n",
    "<ol>\n",
    "  <li>Explore this labeled dataset: <a href=\"https://www.kaggle.com/datatattle/covid-19-nlp-text-classification\">Coronavirus tweets NLP - Text Classification</a></li>\n",
    "  <li>Understand the Naive Bayes based text classification implemented here: <a href=\"https://www.kaggle.com/jayantawasthi/nlp-corona-tweet-with-random-forest-and-naivebayes\">nlp(corona tweet)with random forest and NaiveBayes</a></li>\n",
    "  <li>Create a copy of the above Jupyter notebook (or export it as python program).  In this new notebook (or program) add a detailed description (and in your own words) of what is happening in each code block. (You can skip the steps 27-32 that are related to Random Forest classifier.)\n",
    "</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:09:59.962801Z",
     "start_time": "2021-03-16T20:09:58.548345Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import re\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import xgboost\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "from typing import Tuple, Any\n",
    "from IPython.display import display\n",
    "import sys\n",
    "import concurrent\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:10:00.100801Z",
     "start_time": "2021-03-16T20:09:59.963801Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make a training dataset pd.df of the Corona_NLP_train.csv\n",
    "\n",
    "\"\"\"\n",
    "train = pd.read_csv(\"Corona_NLP_train.csv\", encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:10:00.115801Z",
     "start_time": "2021-03-16T20:10:00.103802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Show first 5 rows of the training dataset\n",
    "\n",
    "\"\"\"\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:10:00.130801Z",
     "start_time": "2021-03-16T20:10:00.116803Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a function to remove irrelavent data (remove irrelavent columns) \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def drop(p):\n",
    "    p.drop([\"UserName\",\n",
    "            \"ScreenName\",\n",
    "            \"Location\",\n",
    "            \"TweetAt\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:10:00.146801Z",
     "start_time": "2021-03-16T20:10:00.131801Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Call the \"remove irrelavent data (remove irrelavent columns)\" function on the training data set\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "drop(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:10:00.161801Z",
     "start_time": "2021-03-16T20:10:00.148801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral\n",
       "1  advice Talk to your neighbours family to excha...            Positive\n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive\n",
       "3  My food stock is not the only one which is emp...            Positive\n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Show first 5 rows of the training dataset\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:10:00.176801Z",
     "start_time": "2021-03-16T20:10:00.163801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive              11422\n",
       "Negative               9917\n",
       "Neutral                7713\n",
       "Extremely Positive     6624\n",
       "Extremely Negative     5481\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Count the amount rows by their given sentiment\n",
    "\n",
    "\"\"\"\n",
    "train[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:10:00.192803Z",
     "start_time": "2021-03-16T20:10:00.177801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41157"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Total amount of rows\n",
    "\n",
    "\"\"\"\n",
    "len(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:10:00.208801Z",
     "start_time": "2021-03-16T20:10:00.193802Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that takes a pd.df and replaces the sentiment column's values (strings) to int.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def rep(t):\n",
    "    d = {\"Sentiment\": {'Positive': 0,\n",
    "                       'Negative': 1,\n",
    "                       \"Neutral\": 2,\n",
    "                       \"Extremely Positive\": 3,\n",
    "                       \"Extremely Negative\": 4}}\n",
    "    t.replace(d, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:10:00.238803Z",
     "start_time": "2021-03-16T20:10:00.209803Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Call replace function on the training dataset to clean it\n",
    "\n",
    "\"\"\"\n",
    "rep(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:10:00.253803Z",
     "start_time": "2021-03-16T20:10:00.241811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet  Sentiment\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...          2\n",
       "1  advice Talk to your neighbours family to excha...          0\n",
       "2  Coronavirus Australia: Woolworths to give elde...          0\n",
       "3  My food stock is not the only one which is emp...          0\n",
       "4  Me, ready to go at supermarket during the #COV...          4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Show first 5 rows of the training dataset\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:10:00.268802Z",
     "start_time": "2021-03-16T20:10:00.255802Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make a nltk TweetTokenizer object to tokenize the tweet.\n",
    "Basically use a custom regex patterns to split the words/special symbols/emojis/etc... from a given tweet.\n",
    "\n",
    "Notes:\n",
    "    Interesting that the nltk library had a Tweet tokenizer...\n",
    "    \n",
    "Reference:\n",
    "    https://www.nltk.org/api/nltk.tokenize.html\n",
    "\"\"\"\n",
    "tweettoken = TweetTokenizer(strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:10:00.283801Z",
     "start_time": "2021-03-16T20:10:00.269801Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make a nltk WordNetLemmatizer to get a meaningful base word from a given word\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:10:00.298802Z",
     "start_time": "2021-03-16T20:10:00.284802Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make a nltk PorterStemmer to get a non meaningful base string of chars from a given word\n",
    "\n",
    "\"\"\"\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:10:00.313801Z",
     "start_time": "2021-03-16T20:10:00.299802Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that takes in a line of text (tweet)\n",
    "1. Uses regex to replace all non Alphabet words with space\n",
    "2. Lowercase each word\n",
    "3. Uses the TweetTokenizer object to tokenize the tweet into a list\n",
    "4. Remove english stop words in the list \n",
    "5. Lemmatize the words in the list into a new list\n",
    "6. Make a string from the new list that contains the lemmatized words\n",
    "7. Add new string into a list of strings (list is called collect)\n",
    "\"\"\"\n",
    "\n",
    "# List of tokenized, lemmatized, non stopping words strings\n",
    "collect = []\n",
    "\n",
    "\n",
    "def preprocess(t):\n",
    "\n",
    "    # Regex remove non alphabet\n",
    "    tee = re.sub('[^a-zA-Z]', \" \", t)\n",
    "\n",
    "    # Lowercase each word\n",
    "    tee = tee.lower()\n",
    "\n",
    "    # Tokenize\n",
    "    res = tweettoken.tokenize(tee)\n",
    "\n",
    "    # Remove enlglish stop words\n",
    "    for i in res:\n",
    "        if i in stopwords.words('english'):\n",
    "            res.remove(i)\n",
    "\n",
    "    # Make new list for words\n",
    "    rest = []\n",
    "\n",
    "    # Add lemmatized word into new list\n",
    "    for k in res:\n",
    "        rest.append(lemmatizer.lemmatize(k))\n",
    "\n",
    "    # Make list into string\n",
    "    ret = \" \".join(rest)\n",
    "\n",
    "    # Add string into list of strings\n",
    "    collect.append(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:12:38.318827Z",
     "start_time": "2021-03-16T20:10:00.314801Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For each each tweet, preprocess that tweet using index \n",
    "\n",
    "Notes:\n",
    "    Cell duration: 2:38 minutes\n",
    "    Can be threaded...\n",
    "\"\"\"\n",
    "for j in range(len(train.index)):\n",
    "    preprocess(train[\"OriginalTweet\"].iloc[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:12:38.334829Z",
     "start_time": "2021-03-16T20:12:38.320829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. menyrbie phil gahan chrisitv http co ifz fan pa http co xx ghgfzcc http co nlzdxno \n",
      "\n",
      "2. advice talk your neighbour family exchange phone number create contact list phone number neighbour school employer chemist gp set online shopping account po adequate supply regular med not order \n",
      "\n",
      "3. coronavirus australia woolworth give elderly disabled dedicated shopping hour amid covid outbreak http co binca vp p \n",
      "\n",
      "4. food stock not only one is empty please panic will enough food everyone not take than you need stay calm stay safe covid france covid covid coronavirus confinement confinementotal confinementgeneral http t co zrlg z j \n",
      "\n",
      "5. ready go supermarket covid outbreak because m paranoid because food stock litteraly empty the coronavirus a serious thing please panic cause shortage coronavirusfrance restezchezvous stayathome confinement http t co usmualq n \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Print the first 5 strings from collect\n",
    "\"\"\"\n",
    "for i, text in enumerate(collect[:5]):\n",
    "    print(f\"{i+1}.\", text, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:12:38.350827Z",
     "start_time": "2021-03-16T20:12:38.336828Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that uses sklearn CountVectorizer to count the amount of words per given tweet by using the\n",
    "List of tokenized, lemmatized, non stopping words strings AKA the list called collect as a word bank.\n",
    "Then this function will return the vector representation (word count for each word, though the word is not shown)\n",
    "for each tweet as an array of arrays\n",
    "\n",
    "Reference:\n",
    "    scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def bow(ll) -> Tuple[Any, CountVectorizer]:\n",
    "    cv = CountVectorizer(max_features=200)\n",
    "    x = cv.fit_transform(ll).toarray()\n",
    "    return x, cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning y using CountVectorizer on the list called \"collect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:12:39.171361Z",
     "start_time": "2021-03-16T20:12:38.352829Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Call bow function on the collection of tweets (List of tokenized, lemmatized, non stopping words strings)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "y, cv = bow(collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:12:39.187360Z",
     "start_time": "2021-03-16T20:12:39.172360Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Show the first tweet in its vector representation\n",
    "\n",
    "\"\"\"\n",
    "y[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:12:39.232360Z",
     "start_time": "2021-03-16T20:12:39.190359Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>also</th>\n",
       "      <th>amid</th>\n",
       "      <th>amp</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>around</th>\n",
       "      <th>at</th>\n",
       "      <th>back</th>\n",
       "      <th>bank</th>\n",
       "      <th>be</th>\n",
       "      <th>been</th>\n",
       "      <th>business</th>\n",
       "      <th>buy</th>\n",
       "      <th>buying</th>\n",
       "      <th>can</th>\n",
       "      <th>care</th>\n",
       "      <th>case</th>\n",
       "      <th>chain</th>\n",
       "      <th>change</th>\n",
       "      <th>check</th>\n",
       "      <th>co</th>\n",
       "      <th>come</th>\n",
       "      <th>company</th>\n",
       "      <th>consumer</th>\n",
       "      <th>corona</th>\n",
       "      <th>coronacrisis</th>\n",
       "      <th>coronavirus</th>\n",
       "      <th>could</th>\n",
       "      <th>country</th>\n",
       "      <th>covid</th>\n",
       "      <th>crisis</th>\n",
       "      <th>customer</th>\n",
       "      <th>day</th>\n",
       "      <th>delivery</th>\n",
       "      <th>demand</th>\n",
       "      <th>distancing</th>\n",
       "      <th>do</th>\n",
       "      <th>don</th>\n",
       "      <th>due</th>\n",
       "      <th>economy</th>\n",
       "      <th>employee</th>\n",
       "      <th>empty</th>\n",
       "      <th>essential</th>\n",
       "      <th>even</th>\n",
       "      <th>every</th>\n",
       "      <th>everyone</th>\n",
       "      <th>face</th>\n",
       "      <th>family</th>\n",
       "      <th>find</th>\n",
       "      <th>first</th>\n",
       "      <th>food</th>\n",
       "      <th>for</th>\n",
       "      <th>free</th>\n",
       "      <th>gas</th>\n",
       "      <th>get</th>\n",
       "      <th>getting</th>\n",
       "      <th>global</th>\n",
       "      <th>go</th>\n",
       "      <th>going</th>\n",
       "      <th>good</th>\n",
       "      <th>got</th>\n",
       "      <th>government</th>\n",
       "      <th>grocery</th>\n",
       "      <th>hand</th>\n",
       "      <th>have</th>\n",
       "      <th>health</th>\n",
       "      <th>help</th>\n",
       "      <th>high</th>\n",
       "      <th>home</th>\n",
       "      <th>hour</th>\n",
       "      <th>how</th>\n",
       "      <th>http</th>\n",
       "      <th>impact</th>\n",
       "      <th>in</th>\n",
       "      <th>increase</th>\n",
       "      <th>industry</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>item</th>\n",
       "      <th>job</th>\n",
       "      <th>just</th>\n",
       "      <th>keep</th>\n",
       "      <th>know</th>\n",
       "      <th>last</th>\n",
       "      <th>let</th>\n",
       "      <th>life</th>\n",
       "      <th>like</th>\n",
       "      <th>line</th>\n",
       "      <th>local</th>\n",
       "      <th>lockdown</th>\n",
       "      <th>long</th>\n",
       "      <th>look</th>\n",
       "      <th>low</th>\n",
       "      <th>make</th>\n",
       "      <th>many</th>\n",
       "      <th>market</th>\n",
       "      <th>mask</th>\n",
       "      <th>may</th>\n",
       "      <th>money</th>\n",
       "      <th>month</th>\n",
       "      <th>more</th>\n",
       "      <th>much</th>\n",
       "      <th>my</th>\n",
       "      <th>need</th>\n",
       "      <th>new</th>\n",
       "      <th>news</th>\n",
       "      <th>no</th>\n",
       "      <th>not</th>\n",
       "      <th>now</th>\n",
       "      <th>of</th>\n",
       "      <th>oil</th>\n",
       "      <th>on</th>\n",
       "      <th>one</th>\n",
       "      <th>online</th>\n",
       "      <th>open</th>\n",
       "      <th>order</th>\n",
       "      <th>other</th>\n",
       "      <th>our</th>\n",
       "      <th>outbreak</th>\n",
       "      <th>pandemic</th>\n",
       "      <th>panic</th>\n",
       "      <th>paper</th>\n",
       "      <th>people</th>\n",
       "      <th>please</th>\n",
       "      <th>price</th>\n",
       "      <th>product</th>\n",
       "      <th>public</th>\n",
       "      <th>quarantine</th>\n",
       "      <th>re</th>\n",
       "      <th>read</th>\n",
       "      <th>really</th>\n",
       "      <th>report</th>\n",
       "      <th>retail</th>\n",
       "      <th>right</th>\n",
       "      <th>risk</th>\n",
       "      <th>safe</th>\n",
       "      <th>said</th>\n",
       "      <th>sanitizer</th>\n",
       "      <th>say</th>\n",
       "      <th>see</th>\n",
       "      <th>service</th>\n",
       "      <th>shelf</th>\n",
       "      <th>shop</th>\n",
       "      <th>shopping</th>\n",
       "      <th>since</th>\n",
       "      <th>social</th>\n",
       "      <th>socialdistancing</th>\n",
       "      <th>some</th>\n",
       "      <th>spread</th>\n",
       "      <th>staff</th>\n",
       "      <th>state</th>\n",
       "      <th>stay</th>\n",
       "      <th>still</th>\n",
       "      <th>stock</th>\n",
       "      <th>stop</th>\n",
       "      <th>store</th>\n",
       "      <th>supermarket</th>\n",
       "      <th>supply</th>\n",
       "      <th>support</th>\n",
       "      <th>take</th>\n",
       "      <th>thank</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>their</th>\n",
       "      <th>there</th>\n",
       "      <th>these</th>\n",
       "      <th>they</th>\n",
       "      <th>thing</th>\n",
       "      <th>think</th>\n",
       "      <th>this</th>\n",
       "      <th>time</th>\n",
       "      <th>to</th>\n",
       "      <th>today</th>\n",
       "      <th>toilet</th>\n",
       "      <th>toiletpaper</th>\n",
       "      <th>uk</th>\n",
       "      <th>use</th>\n",
       "      <th>ve</th>\n",
       "      <th>via</th>\n",
       "      <th>virus</th>\n",
       "      <th>wa</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>we</th>\n",
       "      <th>week</th>\n",
       "      <th>well</th>\n",
       "      <th>went</th>\n",
       "      <th>what</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>work</th>\n",
       "      <th>worker</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>year</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   all  also  amid  amp  an  and  are  around  at  back  bank  be  been  \\\n",
       "0    0     0     0    0   0    0    0       0   0     0     0   0     0   \n",
       "\n",
       "   business  buy  buying  can  care  case  chain  change  check  co  come  \\\n",
       "0         0    0       0    0     0     0      0       0      0   3     0   \n",
       "\n",
       "   company  consumer  corona  coronacrisis  coronavirus  could  country  \\\n",
       "0        0         0       0             0            0      0        0   \n",
       "\n",
       "   covid  crisis  customer  day  delivery  demand  distancing  do  don  due  \\\n",
       "0      0       0         0    0         0       0           0   0    0    0   \n",
       "\n",
       "   economy  employee  empty  essential  even  every  everyone  face  family  \\\n",
       "0        0         0      0          0     0      0         0     0       0   \n",
       "\n",
       "   find  first  food  for  free  gas  get  getting  global  go  going  good  \\\n",
       "0     0      0     0    0     0    0    0        0       0   0      0     0   \n",
       "\n",
       "   got  government  grocery  hand  have  health  help  high  home  hour  how  \\\n",
       "0    0           0        0     0     0       0     0     0     0     0    0   \n",
       "\n",
       "   http  impact  in  increase  industry  is  it  item  job  just  keep  know  \\\n",
       "0     3       0   0         0         0   0   0     0    0     0     0     0   \n",
       "\n",
       "   last  let  life  like  line  local  lockdown  long  look  low  make  many  \\\n",
       "0     0    0     0     0     0      0         0     0     0    0     0     0   \n",
       "\n",
       "   market  mask  may  money  month  more  much  my  need  new  news  no  not  \\\n",
       "0       0     0    0      0      0     0     0   0     0    0     0   0    0   \n",
       "\n",
       "   now  of  oil  on  one  online  open  order  other  our  outbreak  pandemic  \\\n",
       "0    0   0    0   0    0       0     0      0      0    0         0         0   \n",
       "\n",
       "   panic  paper  people  please  price  product  public  quarantine  re  read  \\\n",
       "0      0      0       0       0      0        0       0           0   0     0   \n",
       "\n",
       "   really  report  retail  right  risk  safe  said  sanitizer  say  see  \\\n",
       "0       0       0       0      0     0     0     0          0    0    0   \n",
       "\n",
       "   service  shelf  shop  shopping  since  social  socialdistancing  some  \\\n",
       "0        0      0     0         0      0       0                 0     0   \n",
       "\n",
       "   spread  staff  state  stay  still  stock  stop  store  supermarket  supply  \\\n",
       "0       0      0      0     0      0      0     0      0            0       0   \n",
       "\n",
       "   support  take  thank  that  the  their  there  these  they  thing  think  \\\n",
       "0        0     0      0     0    0      0      0      0     0      0      0   \n",
       "\n",
       "   this  time  to  today  toilet  toiletpaper  uk  use  ve  via  virus  wa  \\\n",
       "0     0     0   0      0       0            0   0    0   0    0      0   0   \n",
       "\n",
       "   want  way  we  week  well  went  what  will  with  work  worker  working  \\\n",
       "0     0    0   0     0     0     0     0     0     0     0       0        0   \n",
       "\n",
       "   world  would  year  you  your  \n",
       "0      0      0     0    0     0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Show the first tweet in its vector representation with its corresponding word\n",
    "\n",
    "Notes:\n",
    "    I made it because I want to see the words and their counts - Joseph\n",
    "\"\"\"\n",
    "\n",
    "pd_temp = pd.DataFrame(y[:1], columns=cv.get_feature_names())\n",
    "pd.options.display.max_columns = len(pd_temp.columns)\n",
    "\n",
    "pd_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:12:39.247360Z",
     "start_time": "2021-03-16T20:12:39.234361Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>co</th>\n",
       "      <th>http</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   co  http\n",
       "0   3     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Show any given tweet in its vector representation with its corresponding word if the count is not 0\n",
    "\n",
    "Notes:\n",
    "    I made it because I want to see the words and their counts clearly - Joseph\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def print_row_with_col_useful(df, index_row=0):\n",
    "    display(df[[word for word, count in zip(\n",
    "        df.columns, df.loc[index_row]) if count]])\n",
    "\n",
    "\n",
    "print_row_with_col_useful(pd_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:12:39.263357Z",
     "start_time": "2021-03-16T20:12:39.248360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Show the amount of features (words) of the first tweet vector representation\n",
    "\n",
    "\"\"\"\n",
    "len(y[0][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:12:39.279357Z",
     "start_time": "2021-03-16T20:12:39.264359Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assign values as the values in the Sentiment column\n",
    "\"\"\"\n",
    "values = train[\"Sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:12:39.294360Z",
     "start_time": "2021-03-16T20:12:39.280357Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, ..., 0, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Display the values array\n",
    "\n",
    "\"\"\"\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:12:39.325361Z",
     "start_time": "2021-03-16T20:12:39.295359Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Call the train_test_split function to split the dataset of the vector representations (the x) and the\n",
    "sentiment values aka \"values\" variable (the y) where 75% of the dataset will be used for training using a random seed\n",
    "to split the data.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "(x_train, x_test, y_train, y_test) = train_test_split(\n",
    "    y, values, train_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:12:39.341361Z",
     "start_time": "2021-03-16T20:12:39.326361Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Display the training data (it should be the y that came from the bow function)\n",
    "\"\"\"\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:12:39.357363Z",
     "start_time": "2021-03-16T20:12:39.343359Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make a Random Forests classifier with 200 trees and use random seed\n",
    "\n",
    "Notes:\n",
    "    It makes a bunch of random decision trees and uses the general consensus of all the trees\n",
    "    to determine the classification of a given thing\n",
    "\n",
    "Reference:\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "    \n",
    "\"\"\"\n",
    "rnd_clf = RandomForestClassifier(n_estimators=200, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:12:57.730496Z",
     "start_time": "2021-03-16T20:12:39.358360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, random_state=42)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train the Random Forests classifier on the training dataset of x and y\n",
    "\n",
    "Notes:\n",
    "    Cell duration: 18.3 seconds\n",
    "\n",
    "\"\"\"\n",
    "rnd_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:12:58.297496Z",
     "start_time": "2021-03-16T20:12:57.731497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4172983479105928"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run the Random Forests classifier on the testing dataset of x and y to see how accurate the classifer is\n",
    "\n",
    "Notes:\n",
    "    Cell duration 2:56 minutes\n",
    "    \n",
    "\"\"\"\n",
    "rnd_clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:12:58.866994Z",
     "start_time": "2021-03-16T20:12:58.298496Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1358,  592,  501,  338,  109],\n",
       "       [ 702, 1014,  448,  106,  243],\n",
       "       [ 477,  438,  913,   46,   45],\n",
       "       [ 653,  183,  162,  617,   28],\n",
       "       [ 287,  457,  148,   33,  392]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use the Random Forests classifier to predict the testing dataset y given testing dataset x.\n",
    "Then use a confusion matrix to show the predictions and the truths. It will be used to determine what machine\n",
    "learning algo to use.\n",
    "\n",
    "Basically for the confusion matrix, look at the diagonal, that determines the amount of correct classifications it got\n",
    "\n",
    "Notes:\n",
    "    Cell duration 2:56 minutes\n",
    "\"\"\"\n",
    "y_pred = rnd_clf.predict(x_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:14:50.502283Z",
     "start_time": "2021-03-16T20:12:58.868994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 0.4152575315840622\n",
      "500 0.41438289601554906\n",
      "600 0.41438289601554906\n",
      "700 0.41243926141885323\n",
      "800 0.41564625850340137\n",
      "900 0.4182701652089407\n",
      "1000 0.41778425655976675\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Instead of 1 Random Forests classifier (from above) now make the len(a) amount of Random Forests classifiers\n",
    "with different amount of trees in the forest\n",
    "\n",
    "Notes:\n",
    "    Cell duration: 8:06 minutes\n",
    "    \n",
    "    Cell duration Threaded: 1 minute 53.5 seconds\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "a = [400, 500, 600, 700, 800, 900, 1000]\n",
    "\n",
    "\n",
    "def run_rand_forest_classifer(i):\n",
    "    rnd_clf = RandomForestClassifier(n_estimators=i, random_state=42)\n",
    "    rnd_clf.fit(x_train, y_train)\n",
    "    t = rnd_clf.score(x_test, y_test)\n",
    "#     print(t)\n",
    "    return i, t\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=len(a)) as executor:\n",
    "    results = [executor.submit(run_rand_forest_classifer, i) for i in a]\n",
    "\n",
    "    # type: concurrent.futures.Future\n",
    "    for i in concurrent.futures.as_completed(results):\n",
    "        print(i.result()[0], i.result()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:14:50.608283Z",
     "start_time": "2021-03-16T20:14:50.503283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Make a sklearn MultinomialNB (Naive bayes with mutiple classification) algorithm calssifer and train it on the training\n",
    "dataset of x and y\n",
    "\n",
    "Reference:\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\"\"\"\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:14:50.639282Z",
     "start_time": "2021-03-16T20:14:50.609283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3825072886297376"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Display how accurate the MultinomialNB is to the testing dataset of x and y\n",
    "\"\"\"\n",
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:14:50.654282Z",
     "start_time": "2021-03-16T20:14:50.640284Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make a term frequency inverse document frequency vectorizer  \n",
    "\n",
    "Notes:\n",
    "    Finds how relavent a word is to the document in collection of documents.\n",
    "    In this case, how relavent a word is to a tweet to a all the other tweets.\n",
    "\n",
    "Reference:\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def tfidf(xx) -> Tuple[Any, TfidfVectorizer]:\n",
    "    cv = TfidfVectorizer(max_features=4000)\n",
    "    x = cv.fit_transform(xx).toarray()\n",
    "    return x, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:14:50.670282Z",
     "start_time": "2021-03-16T20:14:50.655284Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Recall what y looks like\n",
    "\n",
    "\"\"\"\n",
    "display(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning y using TfidfVectorizer on the list called \"collect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:14:51.744115Z",
     "start_time": "2021-03-16T20:14:50.671282Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Do tf-idf on the List of tokenized, lemmatized, non stopping words strings (collect)\n",
    "\n",
    "\"\"\"\n",
    "y, cv = tfidf(collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:14:51.760113Z",
     "start_time": "2021-03-16T20:14:51.745116Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'menyrbie phil gahan chrisitv http co ifz fan pa http co xx ghgfzcc http co nlzdxno'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Print both the collect and the y from tfidif\n",
    "\"\"\"\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "display(collect[0])\n",
    "display(y[0])\n",
    "# print(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:14:52.037113Z",
     "start_time": "2021-03-16T20:14:51.761115Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Again, split the dataset of the vector representations (the x) and the sentiment values aka \"values\" variable (the y) \n",
    "where 75% of the dataset will be used for training using a random seed to split the data.\n",
    "\n",
    "\"\"\"\n",
    "(x_train, x_test, y_train, y_test) = train_test_split(\n",
    "    y, values, train_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:15:18.149666Z",
     "start_time": "2021-03-16T20:14:52.038117Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2937803692905734"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Make a Random Forest Classifier and train it on the training dataset and test it against the testing\n",
    "dataset. Then check how accruate the algorithm is.\n",
    "\n",
    "Notes:\n",
    "    Cell Duration: 25.4 seconds \n",
    "\"\"\"\n",
    "rnd_clf = RandomForestClassifier(\n",
    "    n_estimators=200, max_leaf_nodes=8, random_state=42)\n",
    "rnd_clf.fit(x_train, y_train)\n",
    "rnd_clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T20:15:18.474666Z",
     "start_time": "2021-03-16T20:15:18.150671Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46763848396501456"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Make a MultinomialNB Classifier and train it on the training dataset and test it against the testing\n",
    "dataset. Then check how accruate the algorithm is.\n",
    "\n",
    "\"\"\"\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train, y_train)\n",
    "clf.score(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit Anaconda (Only works with Anaconda's Jupyter)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
